{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing took\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data.csv\")\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "print(x)\n",
    "print(y)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking care of missing data - if the missing data is 1%, we can ignore them. If missing data is more, we shud take care of it as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer is a class inside sklearn. now i am gonna make an object(which is an instance of class \"SimpleImputer\" and use it to find missing data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\") #imputer is an object, SimpleImputer is a class defined in \n",
    "#sklearn package\n",
    "imputer.fit(x[:, 1:3]) # selecting the columns where i wanna apply this \n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3]) #reassigning the values of x to the transformed values\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Spain', 'Germany'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding catogorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "ct = ColumnTransformer(transformers =[(\"encoder\", OneHotEncoder(),[0])],   remainder=\"passthrough\" )\n",
    "x = np.array(ct.fit_transform(x)) #the function expects the argument to be np.array\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-0efc2f38591e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-0efc2f38591e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Spliting the data\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 1)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]\n",
      " [1.0 0.0 0.0 1.0 38.77777777777778 52000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling should be applied after the split, as we dont want any interfearance of test data while testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling - some of the models where data is scattered a lot, feature scaling is applied. \n",
    "but in some models like multiple linear regressing, there is no need for feature scaling, since the co-efficients associated with big numbers can be controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling can be done in 2 ways. 1) Standardisation = (x-mean(x))/SD(x), the range of distribution after this will be between (-l, l) \n",
    "2) Normalisation = (x - min(x))/(max(x)-min(x)), range of distribution will be (0,1)\n",
    "here, standardisation will be used, since its more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() #no need to apply this technique on dummy values(one hot encoded values). \n",
    "x_train[:,3:] = sc.fit_transform(x_train[:,3:])#fit will apply the formula and get mean and SD, but transform will make the changes in x_train\n",
    "x_test[:,3:] = sc.transform(x_test[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 -0.6324555320336758 -0.038910211282047996\n",
      "  -0.22960023388015188]\n",
      " [0.0 1.0 0.0 -0.6324555320336758 0.5058327466666259 0.49120534884662787]\n",
      " [1.0 0.0 0.0 1.5811388300841895 -0.3112816902563849 -0.4731156334500103]\n",
      " [1.0 0.0 0.0 1.5811388300841895 -1.809324824615238 -1.6127677034369463]\n",
      " [0.0 1.0 0.0 -0.6324555320336758 1.0505757046152997 1.1048641557626704]\n",
      " [1.0 0.0 1.0 -0.6324555320336758 1.3229471835896367 1.455526331143266]\n",
      " [0.0 1.0 0.0 -0.6324555320336758 -0.7198389087178904 -0.736112264985457]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 -0.6324555320336758 -1.4007676061537326 -1.0867744403660526]\n",
      " [0.0 1.0 0.0 -0.6324555320336758 -0.4474674297435534 0.05287762962088326]\n",
      " [1.0 0.0 0.0 1.5811388300841895 -0.20535944843303158 -1.2621055280563505]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
